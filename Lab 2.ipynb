{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary Libraries\n",
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer to keep only Alpha-Numeric Terms  [a-zA-Z0-9_]\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "global corpus\n",
    "corpus = {}\n",
    "\n",
    "global index\n",
    "index = defaultdict(list)\n",
    "\n",
    "def PreProcess(data):\n",
    "    data = ' '.join(tokenizer.tokenize(data))\n",
    "    data = data.lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Calculate Freequency of All Terms in Entire Corpus\n",
    "def term_freq_corpus(corpus):\n",
    "    term_freq_c = Counter({})\n",
    "    for doc in corpus:\n",
    "        term_freq_c += Counter(corpus[doc])\n",
    "    return term_freq_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Calculate TF,IDF Values of a Term\n",
    "def tfidf_val_calc(term,term_indxs_lst,ln_corpus,term_freq_c):\n",
    "    # tf : Term Freequency of Term in Document, rounded off to 8 decimal places\n",
    "    try:\n",
    "        tf = round(((1 + math.log10(len(term_indxs_lst[term])))),8)\n",
    "    except:\n",
    "        tf = 0\n",
    "    # idf : Inverse Document Freequency of Term in Corpus, rounded off to 8 decimal places\n",
    "    try:\n",
    "        idf = round((math.log10(ln_corpus / term_freq_c[term])),8)\n",
    "    except:\n",
    "        idf = 0\n",
    "    return tf,idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Calculate Query Vector\n",
    "def Query_Vector_Generator(query,index):\n",
    "    query = PreProcess(query)\n",
    "    query = query.split()\n",
    "    query_vector = defaultdict(lambda: 0)\n",
    "    for term in query:\n",
    "        try:\n",
    "            query_vector[term] = index[term][0]\n",
    "        except:\n",
    "            query_vector[term] = 0\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_IndexFn(stopwords_path,stopwords_filename,collection_path):\n",
    "    # Getting StopWords from a File (Optional)\n",
    "    '''\n",
    "    For Windows Users, Use the following line of code to open file  \n",
    "        with open(path+'\\'+filename) as f:\n",
    "    '''\n",
    "    with open(stopwords_path+'/'+stopwords_filename) as f:\n",
    "        global stopwords\n",
    "        stopwords = f.read().split('\\n')\n",
    "        \n",
    "    # Changing Working Directory to the Folder that contains Document Collection \n",
    "    os.chdir(collection_path)\n",
    "\n",
    "    '''\n",
    "    Reading, Pre-Processing Document Collection and Creating Corpus\n",
    "    It is assumed that Document Collection is in '.txt' files\n",
    "\n",
    "    For Windows Users, Use the following line of code to open file  \n",
    "        file_path = f\".\\{file}\"\n",
    "    '''\n",
    "    for file in os.listdir():\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = f\"./{file}\"\n",
    "            with open(file_path, \"r\") as f:\n",
    "                doc_data = f.read()\n",
    "            doc_data = PreProcess(doc_data)\n",
    "            corpus[file[:-4]] = doc_data.split() # [:-4] to Remove '.txt' from FileName which will be used as DocId\n",
    "    \n",
    "    global ln_corpus \n",
    "    ln_corpus = len(corpus)\n",
    "    \n",
    "    term_freq_c = term_freq_corpus(corpus)\n",
    "        # Create Required Index\n",
    "    for docId in corpus:\n",
    "        # term_indxs_lst Stores All Indexes Of a Term in a Document\n",
    "        term_indxs_lst = defaultdict(list) \n",
    "        doc_data = corpus[docId]\n",
    "        # Such Method Avoids Indexing of Same Term for the Same Document\n",
    "        for i,term in enumerate(doc_data):\n",
    "            if term not in stopwords:\n",
    "                # By Convention, Indexing of Terms in a Document Starts at 1\n",
    "                term_indxs_lst[term].append(i+1)\n",
    "        for term in term_indxs_lst:\n",
    "            # Calculate TF,IDF Values for the Term\n",
    "            tf , idf = tfidf_val_calc(term,term_indxs_lst,ln_corpus,term_freq_c)\n",
    "            # Weight = TF-IDF Value of the Term for the Particular Document\n",
    "            wt = round((tf*idf),8)\n",
    "            if len(index[term]) == 0:\n",
    "                index[term].append(idf)\n",
    "            index[term].append([docId, wt, term_indxs_lst[term]])\n",
    "    # Return the Index\n",
    "    return index"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
