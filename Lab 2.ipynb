{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "W3Uso6vFGnPq"
      },
      "outputs": [],
      "source": [
        "# Importing Necessary Libraries\n",
        "import os\n",
        "import nltk\n",
        "import math\n",
        "import random\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer to keep only Alpha-Numeric Terms  [a-zA-Z0-9_]\n",
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "\n",
        "'''\n",
        "    Corpus will be Stored as Dictionary of Format :\n",
        "    {\n",
        "        DocId1 : ['Entire','Document','As','Pre-Processed','Tokenized','List'....]\n",
        "        DocId2 : ['Entire','Document','As','Pre-Processed','Tokenized','List'....]\n",
        "        .\n",
        "        .\n",
        "        .\n",
        "    }\n",
        "'''\n",
        "global corpus\n",
        "corpus = {}\n",
        "'''\n",
        "    Index will be Stored as a Dictionary of Format :\n",
        "    { \n",
        "        term1 : [ idf_term1, [DocId1,tf_DocId1,[index1,index2,...]] , [DocId2,tf_DocId2,[index1,index2,...]] , ... ]\n",
        "        term2 : [ idf_term2, [DocId1,tf_DocId1,[index1,index2,...]] , [DocId2,tf_DocId2,[index1,index2,...]] , ... ]\n",
        "        .\n",
        "        .\n",
        "        .\n",
        "    }\n",
        "'''\n",
        "global index\n",
        "index = defaultdict(list)\n",
        "\n",
        "'''\n",
        "Pre-Process Document Data\n",
        "     - Remove All Punctuations\n",
        "     - Convert to Lower Case\n",
        "'''\n",
        "def PreProcess(data):\n",
        "    data = ' '.join(tokenizer.tokenize(data))\n",
        "    data = data.lower()\n",
        "    return data"
      ],
      "metadata": {
        "id": "srqWDRtdGqfR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Calculate Freequency of All Terms in Entire Corpus\n",
        "def term_freq_corpus(corpus):\n",
        "    term_freq_c = Counter({})\n",
        "    for doc in corpus:\n",
        "        term_freq_c += Counter(corpus[doc])\n",
        "    return term_freq_c\n",
        "    \n",
        "# Function to Calculate TF,IDF Values of a Term\n",
        "def tfidf_val_calc(term,term_indxs_lst,ln_corpus,term_freq_c):\n",
        "    # tf : Term Freequency of Term in Document, rounded off to 8 decimal places\n",
        "    try:\n",
        "        tf = round(((1 + math.log10(len(term_indxs_lst[term])))),8)\n",
        "    except:\n",
        "        tf = 0\n",
        "    # idf : Inverse Document Freequency of Term in Corpus, rounded off to 8 decimal places\n",
        "    try:\n",
        "        idf = round((math.log10(ln_corpus / term_freq_c[term])),8)\n",
        "    except:\n",
        "        idf = 0\n",
        "    return tf,idf\n",
        "\n",
        "# Function to Calculate Query Vector\n",
        "def Query_Vector_Generator(query,index):\n",
        "    query = PreProcess(query)\n",
        "    query = query.split()\n",
        "    query_vector = defaultdict(lambda: 0)\n",
        "    for term in query:\n",
        "        try:\n",
        "            query_vector[term] = index[term][0]\n",
        "        except:\n",
        "            query_vector[term] = 0\n",
        "    return query_vector"
      ],
      "metadata": {
        "id": "1DUubcTTGuaF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Create_IndexFn(stopwords_path,stopwords_filename,collection_path):\n",
        "    # Getting StopWords from a File (Optional)\n",
        "    '''\n",
        "    For Windows Users, Use the following line of code to open file  \n",
        "        with open(path+'\\'+filename) as f:\n",
        "    '''\n",
        "    with open(stopwords_path+'/'+stopwords_filename) as f:\n",
        "        global stopwords\n",
        "        stopwords = f.read().split('\\n')\n",
        "        \n",
        "    # Changing Working Directory to the Folder that contains Document Collection \n",
        "    os.chdir(collection_path)\n",
        "\n",
        "    '''\n",
        "    Reading, Pre-Processing Document Collection and Creating Corpus\n",
        "    It is assumed that Document Collection is in '.txt' files\n",
        "\n",
        "    For Windows Users, Use the following line of code to open file  \n",
        "        file_path = f\".\\{file}\"\n",
        "    '''\n",
        "    for file in os.listdir():\n",
        "        if file.endswith(\".txt\"):\n",
        "            file_path = f\"./{file}\"\n",
        "            with open(file_path, \"r\") as f:\n",
        "                doc_data = f.read()\n",
        "            doc_data = PreProcess(doc_data)\n",
        "            corpus[file[:-4]] = doc_data.split() # [:-4] to Remove '.txt' from FileName which will be used as DocId\n",
        "    \n",
        "    global ln_corpus \n",
        "    ln_corpus = len(corpus)\n",
        "    \n",
        "    term_freq_c = term_freq_corpus(corpus)\n",
        "        # Create Required Index\n",
        "    for docId in corpus:\n",
        "        # term_indxs_lst Stores All Indexes Of a Term in a Document\n",
        "        term_indxs_lst = defaultdict(list) \n",
        "        doc_data = corpus[docId]\n",
        "        # Such Method Avoids Indexing of Same Term for the Same Document\n",
        "        for i,term in enumerate(doc_data):\n",
        "            if term not in stopwords:\n",
        "                # By Convention, Indexing of Terms in a Document Starts at 1\n",
        "                term_indxs_lst[term].append(i+1)\n",
        "        for term in term_indxs_lst:\n",
        "            # Calculate TF,IDF Values for the Term\n",
        "            tf , idf = tfidf_val_calc(term,term_indxs_lst,ln_corpus,term_freq_c)\n",
        "            # Weight = TF-IDF Value of the Term for the Particular Document\n",
        "            wt = round((tf*idf),8)\n",
        "            if len(index[term]) == 0:\n",
        "                index[term].append(idf)\n",
        "            index[term].append([docId, wt, term_indxs_lst[term]])\n",
        "    # Return the Index\n",
        "    return index"
      ],
      "metadata": {
        "id": "MEmyUOGaG6Z_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLSJJzBJHGhH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mDCQh1pHpbE",
        "outputId": "64e41b3a-7983-4a51-b538-4d08002ca7ba"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the Relevant File/Folder Paths\n",
        "stopwords_path = '/content/drive/MyDrive/stopwords'\n",
        "stopwords_filename = 'stopwords.txt'\n",
        "collection_path = '/content/drive/MyDrive/Corpus'\n"
      ],
      "metadata": {
        "id": "_-yFp_pFHLX4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Calculate Cosine Similarity Scores\n",
        "def TopK_CosineScore(query_vector,k,index):\n",
        "    scores = defaultdict(lambda: 0)\n",
        "    for query_term, query_wt in query_vector.items():\n",
        "        if query_term in index:\n",
        "            for doc_id, doc_wt, _ in index[query_term][1:]:\n",
        "                scores[doc_id] += (query_wt * doc_wt)\n",
        "    TopK_Documents = [X[:2] for X in sorted(scores.items(), key=lambda k: k[1], reverse=True)[:k]]\n",
        "    return TopK_Documents"
      ],
      "metadata": {
        "id": "4fN6TmKsHSoo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ExactQuerySearch(query,N):\n",
        "    query_vector = Query_Vector_Generator(query,index)\n",
        "    ans = TopK_CosineScore(query_vector,N,index)\n",
        "    return print(ans)\n"
      ],
      "metadata": {
        "id": "WNaytAo9HY-M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Index \n",
        "index = Create_IndexFn(stopwords_path,stopwords_filename,collection_path)\n"
      ],
      "metadata": {
        "id": "zJVs5ssZHbWn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Relevant Documents\n",
        "query = 'Developing your Zomato business account and profile is a great way to boost your restaurantâ€™s online reputation'\n",
        "k = 10 # Change this to Required Integer Value\n",
        "Results_ExactSearch = ExactQuerySearch(query,k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O0PwNBiHfR_",
        "outputId": "b6a82830-afe7-4657-ecb1-3a3fac865ca0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('zomato', 3.7184616389084053), ('HP', 2.038589570917866), ('sony', 1.2754688253343631), ('flipkart', 1.0502532673712552), ('volkswagen', 1.021562741387777), ('motorola', 1.0170268295300886), ('Uber', 0.9749226868387171), ('reddit', 0.8582411265279385), ('shakespeare', 0.8350557524518226), ('swiggy', 0.29508390909889415)]\n"
          ]
        }
      ]
    }
  ]
}